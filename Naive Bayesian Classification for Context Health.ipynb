{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Implemenetation\n",
    "import os                        \n",
    "import pandas as pd              \n",
    "import re                        \n",
    "import numpy as np                \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Case  Age                                       Text Details  \\\n",
      "0           0   1.0   67  reviewed pleasant lady private outpatient clin...   \n",
      "1           1   1.0   67  reviewed pleasant lady private outpatient clin...   \n",
      "2           2   1.0   67  discussed finding mr b today flexible cystosco...   \n",
      "3           3   2.0   81  thank referring pleasant gentleman private out...   \n",
      "4           4   2.0   81  writing confirm named patient underwent succes...   \n",
      "\n",
      "  Surgery - Yes OR No  word_count  \n",
      "0                   Y          90  \n",
      "1                   Y          81  \n",
      "2                   N          26  \n",
      "3                   N         105  \n",
      "4                   Y          76  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Case', 'Age', 'Text Details', 'Surgery - Yes OR No',\n",
       "       'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df = pd.read_csv(\"/Users/nikhil/pre_processed_patient_diagnostics.csv\")\n",
    "print(patient_df.head()) # this will print the head of data_domain \n",
    "patient_df.columns #to print column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df['Surgery - Yes OR No'] = patient_df['Surgery - Yes OR No'].astype(str) # taking the target column in seperate variable. we will add this to final data frame when pre processing is done\n",
    "data_desc = patient_df[\"Text Details\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(data_desc)):\n",
    "    data_desc[i] = re.sub(\" \\d+\", \" \",data_desc[i]) # this will retain only the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vct = CountVectorizer(analyzer='word',binary =True,token_pattern=r'\\b\\w\\w+\\b',\n",
    "                             lowercase = True,ngram_range=(1,1),stop_words='english',encoding = 'ISO-8859-1',min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the document term matrix\n",
    "data_mat = cnt_vct.fit_transform(data_desc)\n",
    "#print(data_mat)\n",
    "dense = data_mat.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 409)\n"
     ]
    }
   ],
   "source": [
    "# extracting the features and storing it in a different dataset\n",
    "featureset = pd.DataFrame(dense,columns=cnt_vct.get_feature_names())\n",
    "featureset[\"Surgery - Yes OR No\"] = patient_df['Surgery - Yes OR No']\n",
    "classlabel = \"Surgery - Yes OR No\"\n",
    "\n",
    "\n",
    "print(featureset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 409)\n"
     ]
    }
   ],
   "source": [
    "# shuffing the data to avoid baising \n",
    "featureset = featureset.sample(frac = 1)\n",
    "print(featureset.shape)\n",
    "Targetindex = featureset.columns.get_loc(\"Surgery - Yes OR No\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test split out of training (Divided training into test and train sets respectively)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def splitter(trials,data,size):\n",
    "    \n",
    "    model_acc=[]\n",
    "    model_f1=[]\n",
    "    model_recall=[]\n",
    "    model_precision=[]\n",
    "    \n",
    "  \n",
    "    for i in range(0,trials):\n",
    "        dataset_train, dataset_test = train_test_split(data, test_size=size)\n",
    "        dataset_train = pd.DataFrame(dataset_train)\n",
    "        dataset_test = pd.DataFrame(dataset_test)\n",
    "        dataset_train.columns = featureset.columns\n",
    "        dataset_test.columns = featureset.columns\n",
    "        Targetindex = dataset_train.columns.get_loc(\"Surgery - Yes OR No\")\n",
    "        \n",
    "        X = dataset_train.drop(dataset_train.columns[[Targetindex]], axis=1) # all the features except the target variable\n",
    "        y = dataset_train[\"Surgery - Yes OR No\"] # only the target variable\n",
    "        X1 = dataset_test.drop(dataset_test.columns[[Targetindex]], axis=1) # all the features except the target variable\n",
    "        y1 = dataset_test[\"Surgery - Yes OR No\"] # only the target variable\n",
    "        \n",
    "        # label encode the target variable \n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        train_y = encoder.fit_transform(y)\n",
    "        valid_y = encoder.fit_transform(y1)\n",
    "        \n",
    "        clf = MultinomialNB().fit(X, train_y)\n",
    "        \n",
    "        testpred = clf.predict(X1)\n",
    "        \n",
    "        model_acc.append(accuracy_score(valid_y,testpred))\n",
    "        model_f1.append(f1_score(valid_y,testpred,pos_label=1,average=\"binary\"))\n",
    "        model_recall.append(recall_score(valid_y,testpred,pos_label=1,average=\"binary\"))\n",
    "        model_precision.append(precision_score(valid_y,testpred,pos_label=1,average=\"binary\"))\n",
    "        \n",
    "        matrix=confusion_matrix(valid_y,testpred)\n",
    "        print(\"Confusion matrix for split\",i)\n",
    "        print(matrix)\n",
    "        \n",
    "    print(\"Average metrics for Naive Bayes are:\")\n",
    "    \n",
    "    acc_avg=Average(model_acc)\n",
    "    f1_avg=Average(model_f1)\n",
    "    recall_avg=Average(model_recall)\n",
    "    precision_avg=Average(model_precision)\n",
    "\n",
    "    return acc_avg,f1_avg,recall_avg,precision_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for split 0\n",
      "[[ 4  2]\n",
      " [ 1 29]]\n",
      "Confusion matrix for split 1\n",
      "[[ 3  1]\n",
      " [ 3 29]]\n",
      "Confusion matrix for split 2\n",
      "[[ 2  3]\n",
      " [ 3 28]]\n",
      "Confusion matrix for split 3\n",
      "[[ 3  5]\n",
      " [ 1 27]]\n",
      "Confusion matrix for split 4\n",
      "[[ 4  1]\n",
      " [ 2 29]]\n",
      "Confusion matrix for split 5\n",
      "[[ 3  4]\n",
      " [ 4 25]]\n",
      "Confusion matrix for split 6\n",
      "[[ 3  2]\n",
      " [ 3 28]]\n",
      "Confusion matrix for split 7\n",
      "[[ 3  4]\n",
      " [ 4 25]]\n",
      "Confusion matrix for split 8\n",
      "[[ 3  2]\n",
      " [ 1 30]]\n",
      "Confusion matrix for split 9\n",
      "[[ 3  1]\n",
      " [ 1 31]]\n",
      "Average metrics for Naive Bayes are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8666666666666668, 0.920365069198233, 0.9239767731341703, 0.9179514275120504)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter(10,np.array(featureset),0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
